{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d94b0-f0a4-4b66-9d20-002793f1977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation: max(0, x)\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of ReLU: 1 if x > 0, else 0\"\"\"\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: 1 / (1 + exp(-x))\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid: sigmoid(x) * (1 - sigmoid(x))\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Neural Network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01, l2_lambda=0.01):\n",
    "        \"\"\"Initialize the neural network with given layer sizes, learning rate, and L2 regularization.\"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation: compute activations for all layers.\"\"\"\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            self.activations.append(relu(z))\n",
    "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        self.z_values.append(z)\n",
    "        output = sigmoid(z)\n",
    "        self.activations.append(output)\n",
    "        return output\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Binary Cross-Entropy loss with L2 regularization.\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # Avoid log(0)\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        # Add L2 regularization\n",
    "        l2_term = self.l2_lambda * sum(np.sum(w ** 2) for w in self.weights) / (2 * m)\n",
    "        return loss + l2_term\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"Compute accuracy: proportion of correct predictions.\"\"\"\n",
    "        predictions = (y_pred >= 0.5).astype(int)\n",
    "        return np.mean(predictions == y_true)\n",
    "\n",
    "    def backward(self, X, y, outputs):\n",
    "        \"\"\"Backpropagation: compute gradients with L2 regularization.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        dZ = outputs - y\n",
    "        self.d_weights[-1] = (np.dot(self.activations[-2].T, dZ) + self.l2_lambda * self.weights[-1]) / m\n",
    "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        for i in range(len(self.weights) - 2, -1, -1):\n",
    "            dA = np.dot(dZ, self.weights[i + 1].T)\n",
    "            dZ = dA * relu_derivative(self.z_values[i])\n",
    "            self.d_weights[i] = (np.dot(self.activations[i].T, dZ) + self.l2_lambda * self.weights[i]) / m\n",
    "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * self.d_weights[i]\n",
    "            self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
    "\n",
    "    def train(self, X, y, X_val, y_val, epochs, batch_size):\n",
    "        \"\"\"Train the neural network using mini-batch SGD with validation.\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                outputs = self.forward(X_batch)\n",
    "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
    "                self.backward(X_batch, y_batch, outputs)\n",
    "            epoch_loss /= (X.shape[0] / batch_size)\n",
    "            train_loss = self.compute_loss(y, self.forward(X))\n",
    "            val_loss = self.compute_loss(y_val, self.forward(X_val))\n",
    "            train_accuracy = self.compute_accuracy(y, self.forward(X))\n",
    "            val_accuracy = self.compute_accuracy(y_val, self.forward(X_val))\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels (0 or 1).\"\"\"\n",
    "        outputs = self.forward(X)\n",
    "        return (outputs >= 0.5).astype(int)\n",
    "\n",
    "# Data preprocessing and training\n",
    "def main():\n",
    "    # Load dataset\n",
    "    data = pd.read_csv('diabetes.csv')\n",
    "    # Replace invalid zeros with median\n",
    "    for col in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']:\n",
    "        data[col] = data[col].replace(0, data[col].median())\n",
    "    # Features and target\n",
    "    X = data.drop('Outcome', axis=1).values\n",
    "    y = data['Outcome'].values.reshape(-1, 1)\n",
    "    # Standardize features\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    # Initialize and train model\n",
    "    nn = NeuralNetwork([X_train.shape[1], 16, 8, 1], learning_rate=0.01, l2_lambda=0.01)\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = nn.train(X_train, y_train, X_test, y_test, epochs=100, batch_size=32)\n",
    "    # Evaluate\n",
    "    y_pred = nn.predict(X_test)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    # Loss and accuracy curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.plot(train_losses, label='Train Loss')\n",
    "    ax1.plot(val_losses, label='Validation Loss')\n",
    "    ax1.set_title('Loss Curves')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax2.plot(train_accuracies, label='Train Accuracy')\n",
    "    ax2.plot(val_accuracies, label='Validation Accuracy')\n",
    "    ax2.set_title('Accuracy Curves')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
